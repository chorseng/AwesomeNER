{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook would specifically focus on preprocessing the data for our analysis needs. The introduction would be updated with more "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring certain filepaths and parameters to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining directories for reading text files and saving checkpoints\n",
    "\n",
    "# Directory with raw txt-files\n",
    "TEXT_DIR = 'Train/'\n",
    "\n",
    "# Directory for saving checkpoint and metadata\n",
    "MODEL_DIR = 'Checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(filepath, lowercase = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates input data for the model and most common words\n",
    "    to create the lookup dicts.\n",
    "    \n",
    "    Args:\n",
    "        filepath: String. Path to the data file.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    documents = list()\n",
    "    def tokenize(x): return simple_preprocess(x)\n",
    "\n",
    "    # Read in all files in directory\n",
    "    if os.path.isdir(path):\n",
    "        for filename in os.listdir(path):\n",
    "            with open('%s/%s' % (path, filename), encoding='utf-8') as f:\n",
    "                doc = f.read()\n",
    "                doc = clean_doc(doc)\n",
    "                documents.append(tokenize(doc))\n",
    "    \n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    # Remove numbers\n",
    "    doc = re.sub(r\"[0-9]+\", \"\", doc)\n",
    "    # Split in tokens\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuation\n",
    "    tokens = [w.translate(str.maketrans('', '', string.punctuation))\n",
    "              for w in tokens]\n",
    "    # Tokens with less then two characters will be ignored\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    final_tokens =  ' '.join(tokens)\n",
    "    \n",
    "    data = []\n",
    "    entities = []\n",
    "    words = []\n",
    "\n",
    "    for row in tokenized:\n",
    "        # change row[0] to header name of .csv file later\n",
    "        # while testing\n",
    "        if len(row) == 2 and row[0] != '#':\n",
    "            for cat in cats:\n",
    "                if cat in row[1]:\n",
    "                    if lower:\n",
    "                        entities.append((row[0].lower(), cat))\n",
    "                    else:\n",
    "                        entities.append((row[0], cat))\n",
    "\n",
    "            if row[1] == 'O':\n",
    "                if lower:\n",
    "                    entities.append((row[1].lower(), 'O'))\n",
    "\n",
    "                else:\n",
    "                    if use_chars:\n",
    "                        char_array = [ch for ch in row[1]]\n",
    "                        entities.append(((char_array, row[1]), 'O'))\n",
    "                        unique_chars.update(char_array)\n",
    "                        char_array = []\n",
    "                    else:\n",
    "                        entities.append((row[1], 'O'))\n",
    "            if lower:\n",
    "                words.append(row[1].lower())\n",
    "            else:\n",
    "                words.append(row[1])\n",
    "\n",
    "        elif len(row) == 0 and entities!=[]:\n",
    "            data.append(entities)\n",
    "            entities = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bilstm-crf",
   "language": "python",
   "name": "bilstm-crf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
