{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will go through a CRF only named-entity recognition implementation based on finance corpus. The following would be the sequence of the notebook:\n",
    "<br>\n",
    "1. Loading the dataset into a dataframe\n",
    "2. Data Preprocessing\n",
    "3. Extract features from the sentences (Feature Engineering)\n",
    "4. Training a Condtional Random Field model\n",
    "5. Evaluating the trained CRF model\n",
    "6. Optimising the hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.exceptions import UndefinedMetricWarning \n",
    "\n",
    "import warnings\n",
    "import nltk\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, the data is loaded into a Pandas DataFrame. This can be done easily using the read_csv function, specifying that the separator is a space. It's also useful to keep the blank lines, which are helpful later for determining the sentence breaks. <br>\n",
    "<br>\n",
    "Once the data is loaded into a DataFrame, the easy access we have to columns allows a couple of useful things to be done - group the data by the \"ne\" column to see the distributions of each tag, and extract the classes (disregarding 'O' and blank lines with NaN values) as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Tag Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NER data keeping blank lines and adding columns\n",
    "ner_data = pd.read_csv(\"../Data/tag1.csv\", skip_blank_lines=False, encoding=\"utf-8\", index_col=None)\n",
    "ner_data.columns = [\"Token\", \"NE\"]\n",
    "\n",
    "POS_tags =  nltk.pos_tag(ner_data[\"Token\"])\n",
    "POS_List = []\n",
    "\n",
    "for w in POS_tags:\n",
    "    POS_List.append(w[1])\n",
    "    \n",
    "ner_data[\"POS\"] = POS_List\n",
    "    \n",
    "print(ner_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Tag Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tag_distribution = ner_data.groupby(\"NE\").size().reset_index(name='counts')\n",
    "print(tag_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filtering the classes of Named Entity that we do not require in this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(filter(lambda x: x not in [\"O\", np.nan], list(ner_data[\"NE\"].unique())))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentences from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, sentences need to be extracted from the data - it's useful to have the sentences as a list of lists, with each sublist containing the token, POS tag, and NE label for every word token in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentences dictionary and an initial single sentence dictionary\n",
    "sentences, sentence = [], []\n",
    "# Create a progress bar\n",
    "# pbar = pyprind.ProgBar(len(ner_data))\n",
    "# For each row in the NER data...\n",
    "for index, row in ner_data.iterrows():\n",
    "    # If the row is empty (no string in the token column)\n",
    "    if type(row[\"Token\"]) != str:\n",
    "        # If the current sentence is not empty, append it to the sentences and create a new sentence\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "    # Otherwise...\n",
    "    else:\n",
    "        # If the row does not indicate the start of a document, add the token to the current sentence\n",
    "        if type(row[\"Token\"]) != float and type(row[\"POS\"]) != float and type(row[\"NE\"]) != float:\n",
    "            if not row[\"Token\"].startswith(\"-DOCSTART-\"):\n",
    "                sentence.append([row[\"Token\"], row[\"POS\"], row[\"NE\"]])\n",
    "    #pbar.update()\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define a function which would allow us to extract the word features in the sentence. This includes the following:\n",
    "<br>\n",
    "1. Current Parts of Speech Tags\n",
    "2. Previous and Next Parts of Speech Tags\n",
    "3. Current Words\n",
    "4. Previous Words\n",
    "5. Next Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>For now, we have avoided chunking however a little internet research shows us that chunking indeed can improve the accuracy and sensitivity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordFeatures(sentence, iterator):\n",
    "    POS_Tag = sentence[i][1]\n",
    "    Token = sentence[i][0]\n",
    "\n",
    "    # Aggregating a feuature dicitonary based on the features of the current POS and word\n",
    "    \n",
    "    featureDict = { \"POS[:2]\" : POS[:2],\n",
    "                 \"POS\" : POS,\n",
    "                 \"Token.isdigit()\" : Token.isdigit(),\n",
    "                 \"Token.istitle()\" : Token.istitle(),\n",
    "                 \"Token.isupper()\" : Token.isupper(),\n",
    "                 \"Token[-2:]\" : Token[-2:],\n",
    "                 \"Token[-3:]\" : Token[-3:],\n",
    "                 \"Token.lower()\" : Token.lower(),\n",
    "                 \"bias\" : 1.0,\n",
    "    }\n",
    "    \n",
    "    if iterator > 1:\n",
    "        previousWord = sentence[i-1][0]\n",
    "        previousPosTag = sentence[i-1][1]\n",
    "        \n",
    "        # Add characteristics of the sentence's previous word and POS to the feature dictionary\n",
    "        featureDict.update({ \"-1:Token.lower()\": previousWord.lower(),\n",
    "                          \"-1:Token.istitle()\": previousWord.istitle(),\n",
    "                          \"-1:Token.isupper()\": previousWord.isupper(),\n",
    "                          \"-1:POS\": previousPostag,\n",
    "                          \"-1:POS[:2]\": previousPostag[:2],\n",
    "                        })\n",
    "        \n",
    "    # Add \"Beginning of Sentence\" at the start of the dictionary    \n",
    "    else:\n",
    "        featureDict[\"BOS\"] = True\n",
    "    \n",
    "    if iterator < len(sentence)-1:\n",
    "        nextWord = sentence[i+1][0]\n",
    "        nextPos = sentence[i+1][1]\n",
    "        # Add characteristics of the sentence's previous next and POS to the feature dictionary\n",
    "        featureDict.update({ \"+1:Token.lower()\": nextWord.lower(),\n",
    "                          \"+1:Token.istitle()\": nextWord.istitle(),\n",
    "                          \"+1:Token.isupper()\": nextWord.isupper(),\n",
    "                          \"+1:POS\": nextPos,\n",
    "                          \"+1:POS[:2]\": nextPos[:2],\n",
    "                        })\n",
    "        \n",
    "    else:\n",
    "        features[\"EOS\"] = True\n",
    "return features    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
