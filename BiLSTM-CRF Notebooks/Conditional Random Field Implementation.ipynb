{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will go through a CRF only named-entity recognition implementation based on finance corpus. The following would be the sequence of the notebook:\n",
    "<br>\n",
    "1. Data Preprocessing\n",
    "2. Extract features from the sentences (Feature Engineering)\n",
    "3. Training a Condtional Random Field model\n",
    "4. Evaluating the trained CRF model\n",
    "5. Optimising the hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.exceptions import UndefinedMetricWarning \n",
    "\n",
    "import warnings\n",
    "import nltk\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, the data is loaded into a Pandas DataFrame. This can be done easily using the read_csv function, specifying that the separator is a space. It's also useful to keep the blank lines, which are helpful later for determining the sentence breaks. <br>\n",
    "<br>\n",
    "Once the data is loaded into a DataFrame, the easy access we have to columns allows a couple of useful things to be done - group the data by the \"ne\" column to see the distributions of each tag, and extract the classes (disregarding 'O' and blank lines with NaN values) as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Tag Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            Bought\n",
      "1            credit\n",
      "2           default\n",
      "3        protection\n",
      "4                on\n",
      "5        RadioShack\n",
      "6             Corp.\n",
      "7               and\n",
      "8               pay\n",
      "9             1.16%\n",
      "10          Broker,\n",
      "11              UBS\n",
      "12          Warburg\n",
      "13          Expires\n",
      "14         December\n",
      "15             2010\n",
      "16         USD7,625\n",
      "17           Bought\n",
      "18           credit\n",
      "19          default\n",
      "20       protection\n",
      "21               on\n",
      "22          Limited\n",
      "23          Brands,\n",
      "24             Inc.\n",
      "25              and\n",
      "26              pay\n",
      "27            1.065\n",
      "28          Broker,\n",
      "29              UBS\n",
      "           ...     \n",
      "192           equal\n",
      "193              to\n",
      "194           0.12%\n",
      "195      multiplied\n",
      "196              by\n",
      "197             the\n",
      "198        notional\n",
      "199          amount\n",
      "200             and\n",
      "201         receive\n",
      "202            from\n",
      "203        Barclays\n",
      "204            Bank\n",
      "205             plc\n",
      "206            upon\n",
      "207            each\n",
      "208         default\n",
      "209           event\n",
      "210              of\n",
      "211          Pfizer\n",
      "212           Inc.,\n",
      "213             par\n",
      "214           value\n",
      "215              of\n",
      "216             the\n",
      "217    proportional\n",
      "218        notional\n",
      "219         amount.\n",
      "220      2017-03-01\n",
      "221               6\n",
      "Name: Token, Length: 10913, dtype: object\n",
      "            Token                    NE  POS\n",
      "0          Bought  B-Direction of Trade  NNP\n",
      "1          credit                     O   NN\n",
      "2         default                     O   NN\n",
      "3      protection                     O   NN\n",
      "4              on                     O   IN\n",
      "5      RadioShack    B-Reference Entity  NNP\n",
      "6           Corp.    I-Reference Entity  NNP\n",
      "7             and                     O   CC\n",
      "8             pay                     O   VB\n",
      "9           1.16%          B-Fixed Rate   CD\n",
      "10        Broker,                     O  NNP\n",
      "11            UBS        B-Counterparty  NNP\n",
      "12        Warburg        I-Counterparty  NNP\n",
      "13        Expires                     O  VBZ\n",
      "14       December     B-Expiration Date  NNP\n",
      "15           2010     I-Expiration Date   CD\n",
      "16       USD7,625     B-Notional Amount  NNP\n",
      "17         Bought  B-Direction of Trade  NNP\n",
      "18         credit                     O   NN\n",
      "19        default                     O   NN\n",
      "20     protection                     O   NN\n",
      "21             on                     O   IN\n",
      "22        Limited    B-Reference Entity  NNP\n",
      "23        Brands,    I-Reference Entity  NNP\n",
      "24           Inc.    I-Reference Entity  NNP\n",
      "25            and                     O   CC\n",
      "26            pay                     O   VB\n",
      "27          1.065          B-Fixed Rate   CD\n",
      "28        Broker,                     O  NNP\n",
      "29            UBS        B-Counterparty  NNP\n",
      "..            ...                   ...  ...\n",
      "192         equal                     O   JJ\n",
      "193            to                     O   TO\n",
      "194         0.12%          B-Fixed Rate   CD\n",
      "195    multiplied                     O  VBN\n",
      "196            by                     O   IN\n",
      "197           the                     O   DT\n",
      "198      notional                     O   JJ\n",
      "199        amount                     O   NN\n",
      "200           and                     O   CC\n",
      "201       receive                     O   NN\n",
      "202          from                     O   IN\n",
      "203      Barclays        B-Counterparty  NNP\n",
      "204          Bank        I-Counterparty  NNP\n",
      "205           plc        I-Counterparty   IN\n",
      "206          upon                     O   IN\n",
      "207          each                     O   DT\n",
      "208       default                     O   NN\n",
      "209         event                     O   NN\n",
      "210            of                     O   IN\n",
      "211        Pfizer    B-Reference Entity  NNP\n",
      "212         Inc.,    I-Reference Entity  NNP\n",
      "213           par                     O   IN\n",
      "214         value                     O   NN\n",
      "215            of                     O   IN\n",
      "216           the                     O   DT\n",
      "217  proportional                     O   JJ\n",
      "218      notional                     O   JJ\n",
      "219       amount.                     O   NN\n",
      "220    2017-03-01     B-Expiration Date   JJ\n",
      "221             6     I-Expiration Date   CD\n",
      "\n",
      "[10913 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the NER data keeping blank lines and adding columns\n",
    "ner_data = pd.concat([pd.read_csv(f, skip_blank_lines=False, encoding=\"utf-8\", index_col=None, na_values=' ') for f in glob.glob(\"../Data/*.csv\")])\n",
    "ner_data.columns = [\"Token\", \"NE\"]\n",
    "ner_data[\"Token\"] = ner_data[\"Token\"].astype(str)\n",
    "print(ner_data[\"Token\"])\n",
    "POS_tags =  nltk.pos_tag(ner_data[\"Token\"])\n",
    "\n",
    "POS_List = []\n",
    "\n",
    "for w in POS_tags:\n",
    "    POS_List.append(w[1])\n",
    "    \n",
    "ner_data[\"POS\"] = POS_List\n",
    "    \n",
    "print(ner_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Tag Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      NE  counts\n",
      "0         B-Counterparty     261\n",
      "1   B-Direction of Trade     260\n",
      "2      B-Expiration Date     265\n",
      "3           B-Fixed Rate     263\n",
      "4      B-Notional Amount     261\n",
      "5     B-Reference Entity     252\n",
      "6                    B-o      18\n",
      "7         I-Counterparty     511\n",
      "8      I-Expiration Date      85\n",
      "9           I-Fixed Rate       1\n",
      "10     I-Notional Amount       2\n",
      "11    I-Reference Entity     596\n",
      "12                     O    7602\n"
     ]
    }
   ],
   "source": [
    "tag_distribution = ner_data.groupby(\"NE\").size().reset_index(name='counts')\n",
    "print(tag_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filtering the classes of Named Entity that we do not require in this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Direction of Trade', 'B-Reference Entity', 'I-Reference Entity', 'B-Fixed Rate', 'B-Counterparty', 'I-Counterparty', 'B-Expiration Date', 'I-Expiration Date', 'B-Notional Amount', 'I-Notional Amount', 'I-Fixed Rate', 'B-o']\n"
     ]
    }
   ],
   "source": [
    "classes = list(filter(lambda x: x not in [\"O\", np.nan], list(ner_data[\"NE\"].unique())))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentences from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, sentences need to be extracted from the data - it's useful to have the sentences as a list of lists, with each sublist containing the token, POS tag, and NE label for every word token in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentences dictionary and an initial single sentence dictionary\n",
    "sentences, sentence = [], []\n",
    "# Create a progress bar\n",
    "# pbar = pyprind.ProgBar(len(ner_data))\n",
    "# For each row in the NER data...\n",
    "for index, row in ner_data.iterrows():\n",
    "    # If the row is empty (no string in the token column)\n",
    "    if '\\\\' in row[\"Token\"]:\n",
    "        # If the current sentence is not empty, append it to the sentences and create a new sentence\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "    # Otherwise...\n",
    "    else:\n",
    "        # If the row does not indicate the start of a document, add the token to the current sentence\n",
    "        if type(row[\"Token\"]) != float and type(row[\"POS\"]) != float and type(row[\"NE\"]) != float:\n",
    "            sentence.append([row[\"Token\"], row[\"POS\"], row[\"NE\"]])\n",
    "    #pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define a function which would allow us to extract the word features in the sentence. This includes the following:\n",
    "<br>\n",
    "1. Current Parts of Speech Tags\n",
    "2. Previous and Next Parts of Speech Tags\n",
    "3. Current Words\n",
    "4. Previous Words\n",
    "5. Next Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>For now, we have avoided chunking however a little internet research shows us that chunking indeed can improve the accuracy and sensitivity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordFeatures(sentence, iterator):\n",
    "    POS = sentence[iterator][1]\n",
    "    Token = sentence[iterator][0]\n",
    "\n",
    "    # Aggregating a feuature dicitonary based on the features of the current POS and word\n",
    "    \n",
    "    featureDict = { \"POS[:2]\" : POS[:2],\n",
    "                 \"POS\" : POS,\n",
    "                 \"Token.isdigit()\" : Token.isdigit(),\n",
    "                 \"Token.istitle()\" : Token.istitle(),\n",
    "                 \"Token.isupper()\" : Token.isupper(),\n",
    "                 \"Token[-2:]\" : Token[-2:],\n",
    "                 \"Token[-3:]\" : Token[-3:],\n",
    "                 \"Token.lower()\" : Token.lower(),\n",
    "                 \"bias\" : 1.0,\n",
    "    }\n",
    "    \n",
    "    if iterator > 1:\n",
    "        previousWord = sentence[iterator-1][0]\n",
    "        previousPosTag = sentence[iterator-1][1]\n",
    "        \n",
    "        # Add characteristics of the sentence's previous word and POS to the feature dictionary\n",
    "        featureDict.update({ \"-1:Token.lower()\": previousWord.lower(),\n",
    "                          \"-1:Token.istitle()\": previousWord.istitle(),\n",
    "                          \"-1:Token.isupper()\": previousWord.isupper(),\n",
    "                          \"-1:POS\": previousPosTag,\n",
    "                          \"-1:POS[:2]\": previousPosTag[:2],\n",
    "                        })\n",
    "        \n",
    "    # Add \"Beginning of Sentence\" at the start of the dictionary    \n",
    "    else:\n",
    "        featureDict[\"BOS\"] = True\n",
    "    \n",
    "    if iterator < len(sentence)-1:\n",
    "        nextWord = sentence[iterator+1][0]\n",
    "        nextPos = sentence[iterator+1][1]\n",
    "        # Add characteristics of the sentence's previous next and POS to the feature dictionary\n",
    "        featureDict.update({ \"+1:Token.lower()\": nextWord.lower(),\n",
    "                          \"+1:Token.istitle()\": nextWord.istitle(),\n",
    "                          \"+1:Token.isupper()\": nextWord.isupper(),\n",
    "                          \"+1:POS\": nextPos,\n",
    "                          \"+1:POS[:2]\": nextPos[:2],\n",
    "                        })\n",
    "        \n",
    "    else:\n",
    "        featureDict[\"EOS\"] = True\n",
    "    \n",
    "    return featureDict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the word_features function, a list of feature dictionaries for each word token in a sentence can be extracted, corresponding to a list of NE labels for each word token in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a feature dictionary for each word in a given sentence\n",
    "def sentence_features(sentence):\n",
    "    return [extractWordFeatures(sentence, iterator) for iterator in range(len(sentence))]\n",
    "\n",
    "# Return the label (NER tag) for each word in a given sentence\n",
    "def sentence_labels(sentence):\n",
    "    return [label for token, pos, label in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Condtional Random Field model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the predefined functions, X and y can be extracted as lists of feature dictionaries for each word token in each sentence, and as lists of NE labels for each word token in each sentence, respectively. scikit-learn's 'test_train_split' function can then be used to split X and y into training and test sets, split 80% training to 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First token features:\n",
      "---------------------\n",
      "{'Token.lower()': '1,00,00,000', 'Token.isupper()': False, '+1:Token.istitle()': False, 'Token.isdigit()': False, 'Token[-3:]': '000', 'POS[:2]': 'CD', 'BOS': True, 'POS': 'CD', '+1:POS[:2]': 'NN', 'Token.istitle()': False, '+1:Token.lower()': 'usd', '+1:POS': 'NNP', 'bias': 1.0, 'Token[-2:]': '00', '+1:Token.isupper()': True}\n",
      "\n",
      "First token label:\n",
      "------------------\n",
      "B-Notional Amount\n"
     ]
    }
   ],
   "source": [
    "# For each sentence, extract the sentence features as X, and the labels as y\n",
    "X = [sentence_features(sentence) for sentence in sentences]\n",
    "y = [sentence_labels(sentence) for sentence in sentences]\n",
    "\n",
    "# Split X and y into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"First token features:\\n{}\\n{}\".format(\"-\"*21, X_train[0][0]))\n",
    "print(\"\\nFirst token label:\\n{}\\n{}\".format(\"-\"*18, y_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new CRF model\n",
    "crf = CRF(algorithm=\"lbfgs\",\n",
    "          c1=0.1,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=True)\n",
    "\n",
    "# Train the CRF model on the supplied training data\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the trained CRF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model can now be used to make predictions based on the test data, which can in turn be compared to the expected labels from the test data to produce a classification report (precision, recall and F1 scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "B-Direction of Trade       0.96      1.00      0.98        51\n",
      "  B-Reference Entity       0.94      0.98      0.96        51\n",
      "  I-Reference Entity       0.95      0.98      0.97       114\n",
      "        B-Fixed Rate       1.00      1.00      1.00        52\n",
      "      B-Counterparty       1.00      1.00      1.00        52\n",
      "      I-Counterparty       1.00      1.00      1.00        96\n",
      "   B-Expiration Date       1.00      1.00      1.00        52\n",
      "   I-Expiration Date       0.94      1.00      0.97        15\n",
      "   B-Notional Amount       0.98      0.94      0.96        51\n",
      "   I-Notional Amount       0.00      0.00      0.00         1\n",
      "        I-Fixed Rate       0.00      0.00      0.00         0\n",
      "                 B-o       0.00      0.00      0.00         3\n",
      "\n",
      "           micro avg       0.97      0.98      0.98       538\n",
      "           macro avg       0.73      0.74      0.74       538\n",
      "        weighted avg       0.97      0.98      0.98       538\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vvsaripalli/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vvsaripalli/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Use the CRF model to make predictions on the test data\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under this section, we will experiment with different values of C1 and C2 values for the elastic net regularisation. In order to achieve this we will use cross-validated randomised search. To avoid a computationally intensive task, we will limit the iterations to 50 and use a 3-fold cross-validation. This in turn would mean that we are essentially training a 150 models.  \n",
    "<br>\n",
    "Following the optimisation, we can see that lower values (increased regularisation strength) for both C1 and C2 values result in the best performing model - particularly for C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9359328334687578\n",
      "{'c1': 0.1, 'c2': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Set up a parameter grid to experiment with different values for C1 and C2\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = {\"c1\": param_range,\n",
    "              \"c2\": param_range}\n",
    "\n",
    "# Set up a bespoke scorer that will compare the cross validated models according to their F1 scores\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, average='weighted', labels=classes)\n",
    "\n",
    "# Perform a 3-fold cross-validated, randomised search of 50 combinations for different values for C1 and C2\n",
    "rs = RandomizedSearchCV(estimator=crf,\n",
    "                        param_distributions=param_grid,\n",
    "                        scoring=f1_scorer,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_iter=50,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "# Train the models in the randomised search, ignoring any 'UndefinedMetricWarning' that comes up \n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "# Print the model that scored highest in the randomised search, and the parameters it used\n",
    "print(rs.best_score_)\n",
    "print(rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the optimised CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "B-Direction of Trade       0.96      1.00      0.98        51\n",
      "  B-Reference Entity       0.94      0.98      0.96        51\n",
      "  I-Reference Entity       0.95      0.98      0.97       114\n",
      "        B-Fixed Rate       1.00      1.00      1.00        52\n",
      "      B-Counterparty       1.00      1.00      1.00        52\n",
      "      I-Counterparty       1.00      1.00      1.00        96\n",
      "   B-Expiration Date       1.00      1.00      1.00        52\n",
      "   I-Expiration Date       0.94      1.00      0.97        15\n",
      "   B-Notional Amount       0.98      0.94      0.96        51\n",
      "   I-Notional Amount       0.00      0.00      0.00         1\n",
      "        I-Fixed Rate       0.00      0.00      0.00         0\n",
      "                 B-o       0.00      0.00      0.00         3\n",
      "\n",
      "           micro avg       0.97      0.98      0.98       538\n",
      "           macro avg       0.73      0.74      0.74       538\n",
      "        weighted avg       0.97      0.98      0.98       538\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vvsaripalli/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vvsaripalli/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "crf = rs.best_estimator_\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, labels=classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
